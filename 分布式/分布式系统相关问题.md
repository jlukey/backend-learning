### 1. 分布式服务接口的幂等性如何设计?
比如不能重复扣款



> 所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款，不能多插入一条数据，不能将统计值多加了1。这就是幂等性，
>

保证幂等性主要是三点:

+ 对于每个请求必须有一个唯一的标识，举个例子：订单支付请求，肯定得包含订单id，一个订单id最多支付一次。
+ 每次处理完请求之后，必须有一个记录标识这个请求处理过了，比如说常见的方案是在mysql中记录个状态啥的，比如支付之前记录一条这个订单的支付流水，而且支付流水设置为 唯一键。
+ 每次接收请求需要进行判断之前是否处理过，比如说有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。



实际运作过程中，你要结合自己的业务来，比如说用redis用orderId作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。支付一个订单，必须插入一条支付流水，order_id建一个唯一键unique key。你在支付一个订单之前，先插入一条支付流水，order_id就已经进去了。然后写一个标识到redis里面去，set order_id payed，下一次重复请求过来了，先查redis的order_id对应的value，如果是payed就说明已经支付过了，你就别重复支付了。然后其他请求重复支付这个订单的时候，会尝试插入一条支付流水，数据库报错说unique key冲突了，整个事务回滚就可以了。



### 2. 分布式服务接口请求的顺序性如何保证？


可以通过一致性hash负载均衡策略，将比如某一个订单id对应的请求都给分发到同一台机器上去，接着在这台机器上可能还是多线程并发执行的，你还得将同一个订单id对应的请求扔到一个内存队列里去，强制排队，这样来确保他们的顺序性。



但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案。这种问题最好是通过业务避免，比如一个订单的插入和删除操作，能不能合并成一个操作（就是一个删除），避免这种问题的产生。



如果要达到100%的顺序性，需要通过分布式锁实现，比如增加zookeeper锁机制来控制。但是这样会增加系统复杂性。

### 3. zk的使用场景有哪些？
 

（1）**<font style="color:#F5222D;">分布式协调</font>**：这个其实是zk很经典的一个用法，简单来说，就好比，你A系统发送个请求到mq，然后B系统消费之后处理了。那A系统如何知道B系统的处理结果？用zk就可以实现分布式系统之间的协调工作。A系统发送请求之后可以在zk上对某个节点的值注册个监听器，一旦B系统处理完了就修改zk那个节点的值，A立马就可以收到通知，完美解决。

![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618795224747-6853ac83-16db-4ecc-8bd6-a36b1a3acfd5.png)

 

（2）**<font style="color:#F5222D;">分布式锁</font>**：对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行另外一个机器再执行。那么此时就可以使用zk分布式锁，一个机器接收到了请求之后先获取zk上的一把分布式锁，就是可以去创建一个znode，接着执行操作；然后另外一个机器也尝试去创建那个znode，结果发现自己创建不了，因为被别人创建了。。。。那只能等着，等第一个机器执行完了自己再执行。

 ![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618795504263-fd913501-2dcb-4c41-a250-7d3fca70af59.png)

（3）元数据/配置信息管理：zk可以用作很多系统的配置信息的管理，比如kafka、storm等等很多分布式系统都会选用zk来做一些元数据、配置信息的管理，包括dubbo注册中心不也支持zk么。

 

（4）HA高可用性：这个应该是很常见的，比如hadoop、hdfs、yarn等很多大数据系统，都选择基于zk来开发HA高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过zk感知到切换到备用进程。



### 4. 实现分布式锁都有哪些方式？
#### redis分布式锁
官方叫做**<font style="color:#F5222D;background-color:#FFFB8F;">RedLock</font>**算法，是redis官方支持的分布式锁算法。

 

这个分布式锁有3个重要的考量点，互斥（只能有一个客户端获取锁），不能死锁，容错。

 

**方案一：**

最普通的实现方式，就是在redis里创建一个key加锁。

<font style="color:#FA541C;">SET my:lock </font><font style="color:#FA541C;">随机值</font><font style="color:#FA541C;"> NX PX 30000</font><font style="color:#FA541C;">，</font>这个命令，NX的意思是只有key不存在的时候才会设置成功，PX 30000的意思是30秒后自动过期（锁自动释放)。别人创建的时候如果发现已经有了就不能加锁了。

 ![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618800323625-d5bac5bd-67ed-41d1-b71b-05b6bd3f43dc.png)

释放锁就是删除key，但是一般可以用lua脚本删除，判断value一样才删除：

```java
if redis.call("get",KEYS[1]) == ARGV[1] then
return redis.call("del",KEYS[1])
else
    return 0
end
```



为啥要用随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除key的话会有问题，所以得用随机值加上面的lua脚本来释放锁。



这种方案有弊端，因为如果是普通的redis单实例，那就是单点故障。或者是redis普通主从，那redis主从异步复制，如果主节点挂了，key还没同步到从节点，此时从节点切换为主节点，别人就会拿到锁。



方案二：

RedLock算法。

假设有一个redis cluster，有5个redis master实例。然后执行如下步骤获取一把锁：

1. 获取当前时间戳，单位是毫秒。
2. 通过setNX+expire，轮流尝试在每个master节点上创建锁，过期时间较短，一般就几十毫秒。
3. 尝试在大多数节点上建立一个锁，比如5个节点就要求是3个节点（n / 2 +1）
4. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了。
5. 要是锁建立失败了，那么就依次删除这个锁。
6. 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。



这种方案，在大部分节点上加锁成功，其他服务同时来加锁的时候就会失败，因为有大部分节点已经加锁了。并且能避免如果有某一节点宕机，不会影响锁实现。比如5台节点，宕机一台，只要加锁成功3台还是正常的。



#### zookeeper分布式锁
zk分布式锁，其实可以做的比较简单，就是某个节点尝试创建<font style="color:#FA541C;">临时节点</font>znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新枷锁。

![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618800359483-572e24ee-ff0e-4634-a4c2-aa8b6fdcb1f6.png)

> 注：创建临时节点，避免当前机器挂掉之后，这个节点还一直存在，无法释放锁。
>

### 5. redis分布式锁和zk分布式锁的对比
+ <font style="color:#F5222D;background-color:#FCFCCA;">redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。</font>
+ <font style="color:#F5222D;background-color:#FCFCCA;">zk分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。</font>
+ 如果是redis获取锁的那个客户端bug了或者挂了，那么只能等待超时时间之后才能释放锁；而zk的话，因为创建的是临时znode，只要客户端挂了，znode就没了，此时就自动释放锁。

 

<font style="color:#F5222D;">个人认为</font><font style="color:#F5222D;">zk</font><font style="color:#F5222D;">的分布式锁比</font><font style="color:#F5222D;">redis</font><font style="color:#F5222D;">的分布式锁牢靠、而且模型简单</font>。redis分布式锁遍历上锁，计算时间等等。zookeeper的分布式锁语义清晰实现简单。



多个服务竞争同一把锁，这样实现：

```plain
// 传入进去的locksRoot + “/” + productId
// 假设productId代表了一个商品id，比如说1
// locksRoot = locks
多个服务创建多个节点：locks/10000000000，/locks/10000000001，/locks/10000000002
此时在同一个节点locks下进行从小到大排队，最小的先获取锁。
```

如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁，后面的每个人都会去监听排在自己前面的那个人创建的node上，一旦某个人释放了锁，排在自己后面的人就会被zookeeper给通知，一旦被通知了之后，自己就获取到了锁。



### 6. 集群部署时的分布式session如何实现？
#### tomcat + redis
基于tomcat原生的session支持即可，就是用一个叫做Tomcat RedisSessionManager的东西，让所有我们部署的tomcat都将session数据存储到redis即可。

 

在tomcat的配置文件中，配置一下:

```java
<Valve className="com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve" />

<Manager className="com.orangefunction.tomcat.redissessions.RedisSessionManager"
        host="{redis.host}"
        port="{redis.port}"
        database="{redis.dbnum}"
         maxInactiveInterval="60"/>
```

 搞一个类似上面的配置即可，用RedisSessionManager，然后指定redis的host和 port。



 还可以用下面这种方式基于redis哨兵支持的redis高可用集群来保存session数据：

```java
<Valve className="com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve" />
<Manager className="com.orangefunction.tomcat.redissessions.RedisSessionManager"
          sentinelMaster="mymaster"
          sentinels="<sentinel1-ip>:26379,<sentinel2-ip>:26379,<sentinel3-ip>:26379"
          maxInactiveInterval="60"/>
```



#### spring session + redis
分布式会话的这个东西重耦合在tomcat中不是很好，虽然tomcat + redis的方式好用，但是会严重依赖于web容器，不好将代码移植到其他web容器上去，尤其是你要是换了技术栈咋整？比如换成了spring cloud或者是spring boot之类的。还得好好思忖思忖。

 

所以现在比较好的还是基于java一站式解决方案，spring基本上包含了大部分的我们需要使用的框架了，spirng cloud做微服务，spring boot做脚手架，所以用sping session是一个很好的选择。



给sping session配置基于redis来存储session数据，然后配置了一个spring session的过滤器，这样的话，session相关操作都会交给spring session来管理。接着在代码中，就用原生的session操作，就是直接基于spring sesion从redis中获取数据了。

 

实现分布式的会话，有很多种很多种方式，我说的只不过比较常见的两种方式，tomcat + redis早期比较常用；现流行的方式一般都是通过spring session来实现。还有**<font style="color:#FA541C;">spring session+JDBC</font>**也是一个道理。



### 7. 分布式事务了解吗？解决分布式事务的方式？
#### 分布式事务解决方案
1. <font style="color:#F5222D;">二阶段提交协议（2PC）</font>
2. <font style="color:#F5222D;">三阶段提交协议（3PC）</font>
3. <font style="color:#F5222D;">补偿事务（TCC）</font>
4. <font style="color:#F5222D;">消息中间件实现</font>
5. <font style="color:#F5222D;">seata框架</font>

<font style="color:#F5222D;"></font>

#### 2PC（两阶段提交方案/XA方案）
也叫做两阶段提交事务方案。



举个例子，比如说公司里经常tb是吧（team building，就是团建），然后一般会有个tb主席（就是负责组织团建的那个人）。

+ 第一个阶段，一般tb主席会提前一周问一下团队里的每个人，大家伙下周六我们去滑雪+烧烤去吗？这个时候tb主席开始等待每个人的回答，如果所有人都说ok，那么就可以决定一起去这次tb。如果这个阶段里任何一个人回答说，我有事不去了，那么tb主席就会取消这次活动。
+ 第二个阶段，那下周六大家就一起去滑雪+烧烤了。

所以这个就是所谓的XA事务，两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复ok，那么就正式提交事务，在各个数据库上执行操作；如果任何一个数据库回答不ok，那么就回滚事务。

 

这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于**<font style="color:#F5222D;">spring + JTA</font>**就可以搞定，自己随便搜个demo看看就知道了。

 

这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下，现在微服务，一个大的系统分成几百个服务，几十个服务。一般来说，我们的规定和规范，是要求说每个服务只能操作自己对应的一个数据库。如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套。这样的一套服务是没法管理的，没法治理的，经常数据被别人改错，自己的库被别人写挂。如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许你交叉访问别人的数据库！



#### TCC方案（补偿事务）
TCC的全程是：Try、Confirm、Cancel。

这个其实是用到了补偿的概念，分为了三个阶段：

    1）Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留。

    2）Confirm阶段：这个阶段说的是在各个服务中执行实际的操作。

    3）Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。

 

给大家举个例子吧，比如说跨银行转账的时候，要涉及到两个银行的分布式事务，如果用TCC方案来实现，思路是这样的：

    1）Try阶段：先把两个银行账户中的资金给它冻结住就不让操作了。

    2）Confirm阶段：执行实际的转账操作，A银行账户的资金扣减，B银行账户的资金增加。

    3）Cancel阶段：如果任何一个银行的操作执行失败，那么就需要回滚进行补偿，就是比如A银行账户如果已经扣减了，但是B银行账户资金增加失败了，那么就得把A银行账户资金给加回去。

 

这种方案说实话几乎很少用人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。

 

比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，在资金上出现问题。

 

比较适合的场景：这个就是除非你是真的一致性要求太高，是你系统中核心之核心的场景，比如常见的就是资金类的场景，那你可以用TCC方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否ok，不ok就执行补偿/回滚代码。而且最好是你的各个业务执行的时间都比较短。

 

但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码很难维护。



#### 本地消息表
国外的ebay搞出来的这么一套思想。

大概意思是这样：

1）A系统在自己本地事务里操作的同时，插入一条数据到消息表。

2）接着A系统将这个消息发送到MQ中去。

3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息。

4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态。

5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理。

6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止。

 ![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618813367238-027791cf-19d0-4615-81c5-d8d46170b1ef.png)

这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务。这会导致如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用。



#### 可靠消息最终一致性方案
基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务。

大概的意思就是：

1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了。

2）如果这个消息发送成功了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息。

3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务。

4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所以没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。

5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿。

![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618813383631-0ab4a3df-5cb7-46d4-9ee2-7bff6e34ebed.png)

这个方案，读下面的描述更好理解一些：

![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1619414581760-17731bcf-0cc1-4194-8a50-3b910fdb88fe.png)

![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1619414612737-c44d2e03-f4b9-448a-b1d6-8d88e7a4e5e7.png)



 

这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你就用RocketMQ支持的事务，要不你就自己基于该原理在 ActiveMQ 或者 RabbitMQ 自己封装一套类似的逻辑出来，总之思路就是这样子的。



3.2.6之前的版本有上面的所有机制，之后的版本砍掉了好多东西。



#### 最大努力通知方案
这个方案的大致意思就是：

1）系统A本地事务执行完之后，发送个消息到MQ。

2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统B的接口。

3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃。

 ![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618813415841-30c25eca-f845-49b2-8fc1-8fd38128a687.png)

该方案用的比较少。







<font style="color:#FA541C;">你们公司是如何处理分布式事务的？</font>

<font style="color:#FA541C;">回答可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于</font><font style="color:#FA541C;">rabbitmq</font><font style="color:#FA541C;">来实现。</font>

<font style="color:#FA541C;"></font>



**<font style="color:#F5222D;background-color:#FFFB8F;">99%</font>****<font style="color:#F5222D;background-color:#FFFB8F;">的分布式接口调用，不要做分布式事务，直接就是监控（发邮件、发短信）、记录日志（一旦出错，完整的日志）、事后快速的定位、排查出解决方案、修复数据。</font>**

**<font style="color:#F5222D;background-color:#FFFB8F;"></font>**

<font style="color:#121212;">阿里有个seata也可以解决分布式事务，springboot导入对应maven包就可以了，加注解就行了。</font>

<font style="color:#121212;">阿里的seata事物，数据库层面的虚拟表技术。</font>

> <font style="color:#595959;">Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。</font>
>

<font style="color:#F5222D;background-color:#FFFB8F;"></font>

<font style="color:#F5222D;background-color:#FFFB8F;"></font>

### 8. 如何设计一个高并发系统？
其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？

 

我说的浅显一点，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发2000~3000的时候，基本就快完了。所以才有很多公司，刚开始干的时候，技术比较low，结果业务发展太快，有的时候系统扛不住压力就挂了。数据库如果瞬间承载每秒5000,8000，甚至上万的并发，一定会宕机，因为比如mysql压根儿就扛不住这么高的并发量。

 

所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千很正常的。如果是什么双十一了之类的，每秒并发几万几十万都有可能。

 

那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题：

 

（1）系统拆分，将一个系统拆分为多个子系统，用微服务来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以抗高并发么。

（2）缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存。毕竟人家redis轻轻松松单机几万的并发是没问题的。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。

（3）MQ，必须得用MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改。那高并发绝对搞挂你的系统，你要是用redis来承载写那肯定不行，人家是缓存，数据随时就被LRU了，数据格式还无比简单，没有事务支持。所以该用mysql还得用mysql啊。那你咋办？用MQ吧，大量的写请求灌入MQ里，排队慢慢玩儿，后边系统消费后慢慢写，控制在mysql承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用MQ来异步写，提升并发性。MQ单机抗几万并发也是ok的。

（4）分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，那么就将一个数据库拆分为多个库，多个库来抗更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高sql执行的性能。

（5）读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。

（6）Elasticsearch，可以考虑用es。es是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来抗更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用es来承载，还有一些全文搜索类的操作，也可以考虑用es来承载。

 

上面的6点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。

 

其实大部分公司，真正看重的，不是说你掌握高并发相关的一些基本的架构知识，架构中的一些技术，RocketMQ、Kafka、Redis、Elasticsearch，高并发这一块，次一等的人才。对一个有几十万行代码的复杂的分布式系统，一步一步架构、设计以及实践过高并发架构的人，这个经验是难能可贵的。



![](https://cdn.nlark.com/yuque/0/2021/png/12493416/1618818326544-7a4d5fa7-a4f8-42d5-82a0-4f744d207e05.png)







